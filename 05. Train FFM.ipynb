{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f51f32-7ec4-4667-aa1c-52dd1b626808",
   "metadata": {},
   "source": [
    "UNDER DEVELOPMENT\n",
    "- all seasons field-aware factorization machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20810e-879d-4169-8d0f-6811184e648f",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e63d7-a27d-4ea7-a271-24912812fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67850b2d-7c4b-42cf-b665-be66693d26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/ffm'):\n",
    "    os.mkdir('data/ffm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba79a7-f474-4394-b2a0-5de62fce86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_files = sorted(glob('data/lineup_scores/*.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001579f-0e9a-4aaf-86aa-35d30886a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_season(season_file, lineup_quantile):\n",
    "    \n",
    "    df = pd.read_pickle(season_file)\n",
    "\n",
    "    # group the same exact lineups, to get an overall season results\n",
    "    df = df.groupby(['off1', 'off2', 'off3', 'off4', 'off5', \n",
    "                     'def1', 'def2', 'def3', 'def4', 'def5']).agg({'seconds' : 'sum', \n",
    "                                                                   'points' : 'sum', \n",
    "                                                                   'season' : 'first'}).reset_index()\n",
    "    \n",
    "    # limit to longer lineup times, reduces the number of 0 scores\n",
    "    df = df[df.seconds > df.seconds.quantile(lineup_quantile)].copy()\n",
    "    \n",
    "    player_time = pd.DataFrame()\n",
    "    player_time['PERSON_ID'] = df[['off1', 'off2', 'off3', 'off4', 'off5', \n",
    "                                   'def1', 'def2', 'def3', 'def4', 'def5']].stack().values\n",
    "    player_time['TIME'] = df.seconds.repeat(10).values\n",
    "    player_time = player_time.groupby('PERSON_ID').TIME.sum().reset_index()\n",
    "\n",
    "    df['off1_time'] = df.merge(player_time, left_on='off1', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['off2_time'] = df.merge(player_time, left_on='off2', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['off3_time'] = df.merge(player_time, left_on='off3', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['off4_time'] = df.merge(player_time, left_on='off4', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['off5_time'] = df.merge(player_time, left_on='off5', right_on='PERSON_ID', how='left')['TIME']\n",
    "\n",
    "    df['def1_time'] = df.merge(player_time, left_on='def1', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['def2_time'] = df.merge(player_time, left_on='def2', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['def3_time'] = df.merge(player_time, left_on='def3', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['def4_time'] = df.merge(player_time, left_on='def4', right_on='PERSON_ID', how='left')['TIME']\n",
    "    df['def5_time'] = df.merge(player_time, left_on='def5', right_on='PERSON_ID', how='left')['TIME']\n",
    "\n",
    "    # prelim analyses,  to avod bad data limit to players that have at least 3200 seconds of playing time\n",
    "    good_rows = df[['off1_time', 'off2_time', 'off3_time', 'off4_time', 'off5_time', \n",
    "                    'def1_time', 'def2_time', 'def3_time', 'def4_time', 'def5_time']].min(1) > 3200\n",
    "    df = df[good_rows].copy()\n",
    "    df.reset_index(inplace=True, drop=True)    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f5341-8c14-41e5-9db6-75ff5a8ff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineup_quantile = 0.5\n",
    "all_df = []\n",
    "\n",
    "for season_file in tqdm(season_files, position=0, leave=True):\n",
    "    \n",
    "    with Pool(1) as pool:\n",
    "        df = pool.starmap(load_season, [(season_file, lineup_quantile)])[0]\n",
    "\n",
    "    all_df.append(df)\n",
    "    \n",
    "all_df = pd.concat(all_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c56e50-dd22-49c6-8a78-afdad6c91a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['off', 'def'] + all_df.season.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec79534-2fc4-4e9d-af5c-76f3b7481167",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df = pd.DataFrame()\n",
    "player_df['PERSON_ID'] = all_df[['off1', 'off2', 'off3', 'off4', 'off5']].unstack().unique()\n",
    "player_df['ID'] = np.arange(len(player_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c4da1-02d0-4dc9-a4de-ae9d5162b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['off1_id'] = all_df.merge(player_df, left_on='off1', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['off2_id'] = all_df.merge(player_df, left_on='off2', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['off3_id'] = all_df.merge(player_df, left_on='off3', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['off4_id'] = all_df.merge(player_df, left_on='off4', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['off5_id'] = all_df.merge(player_df, left_on='off5', right_on='PERSON_ID', how='left')['ID']\n",
    "\n",
    "all_df['def1_id'] = all_df.merge(player_df, left_on='def1', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['def2_id'] = all_df.merge(player_df, left_on='def2', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['def3_id'] = all_df.merge(player_df, left_on='def3', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['def4_id'] = all_df.merge(player_df, left_on='def4', right_on='PERSON_ID', how='left')['ID']\n",
    "all_df['def5_id'] = all_df.merge(player_df, left_on='def5', right_on='PERSON_ID', how='left')['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ec21b-d167-4e2c-a6e1-cf796c703292",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['target'] = power_transform((all_df.points / all_df.seconds).values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1b4e1-4979-43af-a079-e7a8463f94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle('data/ffm/nba_all_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d6cd18-0b4c-4b30-af24-48e2ae409eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021cdfd-9668-49b0-a343-a785245784f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txt(filename, df):\n",
    "    \n",
    "    with open(filename, 'w') as f_out:\n",
    "\n",
    "        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "            season_feature = np.where(np.isin(features, row['season']))[0][0]\n",
    "\n",
    "            line = ''\n",
    "            line += str(row['target']) + ' '\n",
    "\n",
    "            for oi in ['off1_id', 'off2_id', 'off3_id', 'off4_id', 'off5_id']:\n",
    "                oid = row[oi] \n",
    "                line += str(oid) + ':0:1 ' + str(oid) + ':' + str(season_feature) + ':1 '\n",
    "\n",
    "            for di in ['def1_id', 'def2_id', 'def3_id', 'def4_id', 'def5_id']:\n",
    "                did = row[di] \n",
    "                line += str(did) + ':1:1 ' + str(did) + ':' + str(season_feature) + ':1 '\n",
    "\n",
    "            line = line[:-1] + '\\n'\n",
    "\n",
    "            f_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fccd268-e5a1-418e-83b4-4b946310e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_txt('data/ffm/nba_ffm_train.txt', train_df)\n",
    "save_txt('data/ffm/nba_ffm_test.txt', test_df)\n",
    "save_txt('data/ffm/nba_ffm_full_train.txt', all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cf55-8e13-4ac1-972d-94d63a186520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction\n",
    "pred_df = pd.DataFrame()\n",
    "pred_df['PERSON_ID'] = all_df[['off1', 'off2', 'off3', 'off4', 'off5']].stack().values\n",
    "pred_df['LAT_ID'] = all_df[['off1_id', 'off2_id', 'off3_id', 'off4_id', 'off5_id']].stack().values\n",
    "pred_df['season'] = np.repeat(all_df.season.values, 5)\n",
    "pred_df.drop_duplicates(inplace=True)\n",
    "\n",
    "pred_df = pd.concat([pred_df, pred_df], ignore_index=True)\n",
    "pred_df['poss'] = [0] * (len(pred_df)//2) + [1] * (len(pred_df)//2)\n",
    "\n",
    "pred_df.to_pickle('data/ffm/nba_pred_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734ba05-1428-4600-913b-cdcd574e6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_file = 'data/ffm/nba_ffm_pred.txt'\n",
    "\n",
    "with open(pred_file, 'w') as f_out:\n",
    "    \n",
    "    for i, row in tqdm(pred_df.iterrows(), total=len(pred_df)):\n",
    "        \n",
    "        line = '1 '\n",
    "        \n",
    "        season_feature = np.where(np.isin(features, row['season']))[0][0]\n",
    "        \n",
    "        line += str(row['LAT_ID']) + ':' + str(row['poss']) + ':1 '\n",
    "        line += str(row['LAT_ID']) + ':' + str(season_feature) + ':1\\n'\n",
    "        \n",
    "        f_out.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede4808-8013-467e-8ec1-316d4c1a1e89",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a15e778-7040-44b2-9523-abaf8ab3f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlearn\n",
    "from wurlitzer import pipes, STDOUT\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16433ad5-8357-4fe5-b95a-61502c138344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrs = [0.01, 0.02, 0.05, 0.075, 0.1, 0.15, 0.2]\n",
    "#lmbs = [0.00002, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "#ks = [1, 2, 3, 4]\n",
    "\n",
    "lrs = [0.02, 0.05]\n",
    "lmbs = [0.0002, 0.0005]\n",
    "ks = [2, 3]\n",
    "\n",
    "params = [(lr, lmb, k) for lr in lrs for lmb in lmbs for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c84893-1f22-4849-8dc3-8c08ac2451b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, lmb, k):\n",
    "    \n",
    "    train_param = {'task':'reg', 'init': 0.1, 'k':k, 'lr':lr, 'lambda':lmb}\n",
    "\n",
    "    # setting up the FM\n",
    "    fm = xlearn.create_ffm()\n",
    "    fm.setTrain('data/ffm/nba_ffm_train.txt')\n",
    "    fm.setValidate('data/ffm/nba_ffm_test.txt')\n",
    "    \n",
    "    out = io.StringIO()\n",
    "    with pipes(stdout=out, stderr=STDOUT):\n",
    "        fm.cv(train_param)\n",
    "    out_val = out.getvalue()\n",
    "    # get the cv loss\n",
    "    cv_mse = float(out_val.split('Average mse_loss:')[1].split('\\n')[0].strip())\n",
    "    \n",
    "    fm = xlearn.create_ffm()\n",
    "    fm.setTrain('data/ffm/nba_ffm_full_train.txt')\n",
    "    fm.setTXTModel('data/ffm/model.txt')\n",
    "    \n",
    "    out = io.StringIO()\n",
    "    with pipes(stdout=out, stderr=STDOUT):\n",
    "        fm.fit(train_param, 'data/ffm/model.out')\n",
    "    full_out_val = out.getvalue()\n",
    "    \n",
    "    # run prediction\n",
    "    fm = xlearn.create_ffm()\n",
    "    fm.setTest('data/ffm/nba_ffm_pred.txt')\n",
    "    # make predictions\n",
    "    pred_file = 'data/ffm/predict.txt'\n",
    "    out = io.StringIO()\n",
    "    with pipes(stdout=out, stderr=STDOUT):\n",
    "        fm.predict(f'data/ffm/model.out', pred_file)\n",
    "        \n",
    "    pred_values = []\n",
    "    with open(pred_file) as f:\n",
    "        for line in f:\n",
    "            pred_values.append(float(line.replace('\\n', '')))\n",
    "    pred_values = np.asarray(pred_values)\n",
    "    pred_values = (pred_values - pred_values.mean()) / pred_values.std()    \n",
    "    \n",
    "    return cv_mse, pred_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76615916-b04e-4cbf-a28f-3da8f7845002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mse_list = []\n",
    "pred_values_list = []\n",
    "\n",
    "for (lr, lmb, k) in tqdm(params, position=0, leave=True):\n",
    "\n",
    "    filename = 'data/ffm/{lr}_{lmb}_{k}.pkl'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "        \n",
    "    with Pool(1) as pool:\n",
    "        cv_mse, pred_values = pool.starmap(train, [[lr, lmb, k]])[0]\n",
    "        \n",
    "    cv_mse_list.append(cv_mse)\n",
    "    pred_values_list.append(pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03641622-3500-4c4c-8b55-c7c0884745b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_df = []\n",
    "\n",
    "for cv_mse, pred_values in zip(cv_mse_list, pred_values_list):\n",
    "    \n",
    "    pred_df = pd.read_pickle('data/ffm/nba_pred_df.pkl')\n",
    "    \n",
    "    pred_df['pred'] = pred_values\n",
    "    pred_df['cv_mse'] = cv_mse\n",
    "   \n",
    "    all_pred_df.append(pred_df)\n",
    "    \n",
    "all_pred_df = pd.concat(all_pred_df, ignore_index=True)\n",
    "all_pred_df.to_pickle('data/ffm/all_pred_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
