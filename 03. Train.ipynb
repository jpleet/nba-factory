{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11b9b4b-0a69-4b42-904b-66c0dd5ccdb5",
   "metadata": {},
   "source": [
    "# Train Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211e06c-5740-4e90-b162-fe5cecbd135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from wurlitzer import pipes, STDOUT\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import xlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c7bad-7e77-45c2-bda3-3330123cddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/contribution_predictions'):\n",
    "    os.makedirs('data/contribution_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eda23f-4693-4310-a641-3764fe73d87e",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169380b-6c4b-4a31-a657-e5ee5990f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_files = sorted(glob('data/lineup_scores/*.pkl'))\n",
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "#lrs = [0.01, 0.02, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.5]\n",
    "#lmbs = [0.00002, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "lrs = [0.05]\n",
    "lmbs = [0.00002]\n",
    "\n",
    "params = [(lr, lmb) for lr in lrs for lmb in lmbs for _ in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631b064-e818-49fb-b1ec-fee0fa25a04d",
   "metadata": {},
   "source": [
    "## Train ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386cdb63-8adf-48a6-858c-6ea96b1168c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for season_file in tqdm(season_files):\n",
    "\n",
    "    filename = 'data/contribution_predictions/' + season_file.split('/')[-1]\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    \n",
    "    res = []\n",
    "    count = 0\n",
    "    \n",
    "    for quantile in quantiles:\n",
    "        \n",
    "        df = pd.read_pickle(season_file)\n",
    "        \n",
    "        player_time = pd.DataFrame()\n",
    "        player_time['PERSON_ID'] = df[['off1', 'off2','off3', 'off4', 'off5', 'def1', 'def2', 'def3', 'def4', 'def5']].stack().values\n",
    "        player_time['TIME'] = df.seconds.repeat(10).values\n",
    "        player_time = player_time.groupby('PERSON_ID').TIME.sum().reset_index()\n",
    "\n",
    "        df['off1_time'] = df.merge(player_time, left_on='off1', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['off2_time'] = df.merge(player_time, left_on='off2', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['off3_time'] = df.merge(player_time, left_on='off3', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['off4_time'] = df.merge(player_time, left_on='off4', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['off5_time'] = df.merge(player_time, left_on='off5', right_on='PERSON_ID', how='left')['TIME']\n",
    "\n",
    "        df['def1_time'] = df.merge(player_time, left_on='def1', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['def2_time'] = df.merge(player_time, left_on='def2', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['def3_time'] = df.merge(player_time, left_on='def3', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['def4_time'] = df.merge(player_time, left_on='def4', right_on='PERSON_ID', how='left')['TIME']\n",
    "        df['def5_time'] = df.merge(player_time, left_on='def5', right_on='PERSON_ID', how='left')['TIME']\n",
    "        \n",
    "        good_rows = df[['off1_time', 'off2_time', 'off3_time', 'off4_time', 'off5_time', \n",
    "                        'def1_time', 'def2_time', 'def3_time', 'def4_time', 'def5_time']].min(1) > 3200\n",
    "        df = df[good_rows].copy()\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        good_rows = df[['off1_time', 'off2_time', 'off3_time', 'off4_time', 'off5_time', \n",
    "                        'def1_time', 'def2_time', 'def3_time', 'def4_time', 'def5_time']].min(1) > player_time.TIME.quantile(quantile)\n",
    "        df = df[good_rows].copy()\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        player_df = pd.DataFrame()\n",
    "        player_df['PERSON_ID'] = df[['off1', 'off2', 'off3', 'off4', 'off5']].unstack().unique()\n",
    "        player_df['OFF_ID'] = np.arange(len(player_df))\n",
    "        player_df['DEF_ID'] = np.arange(len(player_df)) + len(player_df)\n",
    "        \n",
    "        df['off1_id'] = df.merge(player_df, left_on='off1', right_on='PERSON_ID', how='left')['OFF_ID']\n",
    "        df['off2_id'] = df.merge(player_df, left_on='off2', right_on='PERSON_ID', how='left')['OFF_ID']\n",
    "        df['off3_id'] = df.merge(player_df, left_on='off3', right_on='PERSON_ID', how='left')['OFF_ID']\n",
    "        df['off4_id'] = df.merge(player_df, left_on='off4', right_on='PERSON_ID', how='left')['OFF_ID']\n",
    "        df['off5_id'] = df.merge(player_df, left_on='off5', right_on='PERSON_ID', how='left')['OFF_ID']\n",
    "        \n",
    "        df['def1_id'] = df.merge(player_df, left_on='def1', right_on='PERSON_ID', how='left')['DEF_ID']\n",
    "        df['def2_id'] = df.merge(player_df, left_on='def2', right_on='PERSON_ID', how='left')['DEF_ID']\n",
    "        df['def3_id'] = df.merge(player_df, left_on='def3', right_on='PERSON_ID', how='left')['DEF_ID']\n",
    "        df['def4_id'] = df.merge(player_df, left_on='def4', right_on='PERSON_ID', how='left')['DEF_ID']\n",
    "        df['def5_id'] = df.merge(player_df, left_on='def5', right_on='PERSON_ID', how='left')['DEF_ID']\n",
    "        \n",
    "        target = power_transform((df.points / df.seconds).values.reshape(-1,1)).ravel()\n",
    "        \n",
    "        cols = df[['off1_id', 'off2_id', 'off3_id', 'off4_id', 'off5_id', \n",
    "                   'def1_id', 'def2_id', 'def3_id', 'def4_id', 'def5_id']].stack()\n",
    "        rows = cols.index.get_level_values(0)\n",
    "        z = [1] * len(rows)\n",
    "\n",
    "        mat = sp.csr_matrix((z,(rows, cols)),shape=(len(df), len(player_df)*2))\n",
    "        \n",
    "        predict_file = f'/tmp/nba_predict.txt'\n",
    "        dump_svmlight_file(sp.eye(len(player_df)*2), [1]*(len(player_df)*2), predict_file)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(mat, target, test_size=0.1)\n",
    "    \n",
    "        train_file = f'/tmp/nba_train.txt'\n",
    "        valid_file = f'/tmp/nba_valid.txt'\n",
    "        full_train_file = f'/tmp/nba_full_train.txt'\n",
    "        dump_svmlight_file(X_train, y_train, train_file)\n",
    "        dump_svmlight_file(X_val, y_val, valid_file)\n",
    "        dump_svmlight_file(mat, target, full_train_file)\n",
    "        \n",
    "        for (lr, lmb) in params:\n",
    "            \n",
    "            train_param = {'task':'reg', 'init': 0.1, 'k':1, 'lr':lr, 'lambda':lmb}\n",
    "        \n",
    "            # setting up the FM\n",
    "            fm = xlearn.create_fm()\n",
    "            fm.setTrain(train_file)\n",
    "            fm.setValidate(valid_file)\n",
    "\n",
    "            out = io.StringIO()\n",
    "            with pipes(stdout=out, stderr=STDOUT):\n",
    "                fm.cv(train_param)\n",
    "            out_val = out.getvalue()\n",
    "            # get the cv loss\n",
    "            cv_mse = float(out_val.split('Average mse_loss:')[1].split('\\n')[0].strip())\n",
    "\n",
    "            fm = xlearn.create_fm()\n",
    "            fm.setTrain(full_train_file)\n",
    "            fm.setTXTModel(f'/tmp/model.txt')\n",
    "\n",
    "            out = io.StringIO()\n",
    "            with pipes(stdout=out, stderr=STDOUT):\n",
    "                fm.fit(train_param, f'/tmp/model.out')\n",
    "            full_out_val = out.getvalue()\n",
    "\n",
    "            # run prediction\n",
    "            #fm = xlearn.create_fm()\n",
    "            fm.setTest(predict_file)\n",
    "            # make predictions\n",
    "            pred_file = '/tmp/predict.txt'\n",
    "            out = io.StringIO()\n",
    "            with pipes(stdout=out, stderr=STDOUT):\n",
    "                fm.predict(f'/tmp/model.out', pred_file)\n",
    "\n",
    "            pred_df = pd.DataFrame()\n",
    "            pred_values = []\n",
    "            with open(pred_file) as f:\n",
    "                for line in f:\n",
    "                    pred_values.append(float(line.replace('\\n', '')))\n",
    "            pred_df['contribution'] = pred_values\n",
    "            pred_df['LAT_ID'] = np.arange(len(pred_df))\n",
    "\n",
    "            player_df_temp = player_df.copy()\n",
    "            player_df_temp['off_contribution'] = player_df_temp.merge(pred_df, left_on='OFF_ID', right_on='LAT_ID', how='left')['contribution']\n",
    "            player_df_temp['def_contribution'] = player_df_temp.merge(pred_df, left_on='DEF_ID', right_on='LAT_ID', how='left')['contribution']\n",
    "            \n",
    "            player_df_temp['off_contribution_norm'] = ((player_df_temp.off_contribution - player_df_temp.off_contribution.mean()) / \n",
    "                                                       (player_df_temp.off_contribution.std()))\n",
    "            player_df_temp['def_contribution_norm'] = ((player_df_temp.def_contribution - player_df_temp.def_contribution.mean()) / \n",
    "                                                       (player_df_temp.def_contribution.std()))\n",
    "            \n",
    "            player_df_temp = player_df_temp.merge(player_time, on='PERSON_ID', how='left')\n",
    "            \n",
    "            player_df_temp['cv_mse'] = cv_mse\n",
    "            player_df_temp['quantile'] = quantile\n",
    "            player_df_temp['counter'] = count\n",
    "            count += 1\n",
    "            \n",
    "            res.append(player_df_temp)\n",
    "            \n",
    "    res = pd.concat(res, ignore_index=True)\n",
    "    res['season'] = season_file.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    res.to_pickle(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
